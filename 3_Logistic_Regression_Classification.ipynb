{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3. Logistic Regression: Classification",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arncelik/TAMU-Datathon-Challenges/blob/master/3_Logistic_Regression_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aX6ZhYh5ZDPd",
        "colab_type": "text"
      },
      "source": [
        "# Lesson 3: Logistic Regression, Classification and Accuracy\n",
        "\n",
        "Now that we know what it means to fit a model, we are going to dive into the second type of data prediction which is called **classification**.\n",
        "\n",
        "## Learning Objectives:\n",
        "* What is classification?\n",
        "* What is logistic regression?\n",
        "* A visual introduction to non-convex optimization.\n",
        "* Fit a logistic regression model yourself!\n",
        "* Fit a logistic regression model using the Scikit Learn library.\n",
        "* Fake News!! What is accuracy, true/false positives/negatives, precision, recall?\n",
        "* Generalizing logistic to multiple classes.\n",
        "* Visualize 2D decision boundaries.\n",
        "\n",
        "## Definitions\n",
        "* **Accuracy** - The number of guesses or predictions correct / total number of predictions.\n",
        "* **Ground Truth** - Used in reference to our prediction. A ground truth is the correct value for our our prediction.\n",
        "* **True Positive** - When we predicted 1 and the ground truth was 1.\n",
        "* **True Negative** - When we predicted 0 and the ground truth was 0.\n",
        "* **False Positive** - When we predicted 1 and the ground truth was 0.\n",
        "* **False Negative** - When we predicted 0 and the ground truth was 1.\n",
        "\n",
        "## Challenges\n",
        "1. Classify dummy data\n",
        "2. Classify if a person will survive the titanic and why\n",
        "3. Plot nominal data and try to determine the cutoff\n",
        "4. Plot decision boundaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJYqorfIjcPW",
        "colab_type": "text"
      },
      "source": [
        "### What is Classification?\n",
        "\n",
        "![alt text](https://i0.wp.com/dataaspirant.com/wp-content/uploads/2014/09/Classification-and-Regression-dataaspirant.png?resize=690%2C518)\n",
        "\n",
        "**Classification (as opposed to regression) is used when our target is categorical. i.e. pass/fail.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6HoCDSDxv-w",
        "colab_type": "text"
      },
      "source": [
        "### What is Logistic Regression?\n",
        "\n",
        "Logistic Regression is a Machine Learning classification algorithm that is used to predict the probability of a categorical dependent variable. In logistic regression, the dependent variable is a binary variable that contains data coded as 1 (yes, positive, success, etc.) or 0 (no, negative, failure, etc.). In other words, the logistic regression model predicts P(Y=1) as a function of X.\n",
        "\n",
        "Unlike linear regression, logistic regression can directly predict probabilities (values that are restricted to the (0,1) interval); furthermore, those probabilities are well-calibrated when compared to the probabilities predicted by some other classifiers, such as Naive Bayes. Logistic regression preserves the marginal probabilities of the training data. The coefficients of the model also provide some hint of the relative importance of each input variable.\n",
        "\n",
        "[source](ttps://medium.com/greyatom/logistic-regression-89e496433063)\n",
        "\n",
        "**Applications:**\n",
        "1. Marketing applications such as prediction of a customer’s propensity to purchase a product or halt a subscription.\n",
        "2. Model whether the customer is going to “Default” or “Not Default” on a credit card.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW7c_6LWyL22",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://miro.medium.com/max/700/0*j4b6G61h6FGvaS16.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHYvf2PB7R4W",
        "colab_type": "text"
      },
      "source": [
        "## Fitting a Logistic Regression Model\n",
        "\n",
        "#### The Model (Sigmoid Function)\n",
        "$$y = \\frac{e^{b+wx}}{1 + e^{b+wx}}$$\n",
        "\n",
        "\n",
        "#### Score Function: Accuracy\n",
        "For now, we will be using **accuracy** as the score function which is defined as:\n",
        "$$\\frac{|\\hat{y} =y|}{|\\hat{y}|}$$\n",
        "\n",
        "In words, accuracy is the number of predictions we got right!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fWu5L-w7sXy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(y_true, y_pred):\n",
        "  return (y_true == y_pred).sum() / len(y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2F7bXSixx8Xd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GpqLk-EaIR8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def logistic_predict_prob(w, b, x):\n",
        "  return (np.e**(b+w*x)) / (1 + np.e**(b+w*x))\n",
        "\n",
        "def logistic_predict_binary(w, b, x):\n",
        "  return logistic_predict_prob(w, b, x) >= 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRUrfQdOWJZ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dummy_data():\n",
        "  x1 = np.sort(np.random.uniform(0, 7, 50))\n",
        "  x2 = np.sort(np.random.uniform(4, 10, 50))\n",
        "  x = np.concatenate([x1, x2])\n",
        "  y = np.concatenate([np.zeros(50), np.ones(50)])\n",
        "  return x, y\n",
        "# Set constants for plotting\n",
        "w_min, w_max, w_step = 0, 3, .5\n",
        "b_min, b_max, b_step = -10, 5, .5\n",
        "w_range = np.linspace(w_min, w_max, 100)\n",
        "b_range = np.linspace(b_min, b_max, 100)\n",
        "x, y = dummy_data()\n",
        "x_range = np.linspace(min(x), max(x), 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHSeso8KV2MX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def last0_idx(a):\n",
        "  last0x = np.where(~a)[0]\n",
        "  return max(last0x) if len(last0x) > 0 else None\n",
        "\n",
        "def plot_model(w, b):\n",
        "    fig = plt.figure(figsize=(8, 6))\n",
        "    plt.scatter(x, y, label=\"Past Users\")\n",
        "    y_hat = logistic_predict_prob(w, b, x_range)\n",
        "    predictions = logistic_predict_binary(w, b, x)\n",
        "    plt.plot(x_range, y_hat, color=\"r\", label='Model - Acc: {}'.format(accuracy(y, predictions)))\n",
        "    plt.plot(x_range, logistic_predict_binary(w, b, x_range), color=\"g\", label='Decision Boundary')\n",
        "    plt.xlabel(\"Months the user has used the service\")\n",
        "    plt.ylabel(\"Probability of user staying on the service\")\n",
        "    if 0 in predictions:\n",
        "      last0x = x_range[last0_idx(y_hat > 0.5)]\n",
        "      plt.annotate(\n",
        "        \"Predict that a user\\n\"\n",
        "        \"with more than {:.1f} months\\n\"\n",
        "        \"using the service will stay.\"\n",
        "        .format(last0x),\n",
        "        (last0x+.3, 0.5)\n",
        "      )\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.title('Fit Model To Data')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CULI-FosWOsC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Some libraries we need for interactive plotting\n",
        "from ipywidgets import interact, interactive, fixed, interact_manual\n",
        "import ipywidgets as widgets\n",
        "from mpl_toolkits import mplot3d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u46YpIZgg0Br",
        "colab_type": "text"
      },
      "source": [
        "### Hand fit a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blaANvCJVxl7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run this block to vizualize how to fit a model\n",
        "# You may have to run this cell a few times to get the plot to show\n",
        "interact(\n",
        "  plot_model,\n",
        "  w = widgets.FloatSlider(value=1, min=w_min, max=w_max, step=w_step),\n",
        "  b = widgets.FloatSlider(value=-8, min=b_min, max=b_max, step=b_step)\n",
        ")\n",
        "print(\"Adjust the parameters: w and b to fit the model to the data!\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfkBZ3XsgqSy",
        "colab_type": "text"
      },
      "source": [
        "### Have the computer fit the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTrJjIg0Rvfh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression(solver=\"lbfgs\").fit(x.reshape(-1, 1), y)\n",
        "w, b = model.coef_[0][0], model.intercept_[0]\n",
        "\n",
        "plot_model(w, b)\n",
        "print(\"Computer Fit Model: w={:.1f}, b={:.1f}\".format(w, b))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OFLUNDphB1B",
        "colab_type": "text"
      },
      "source": [
        "## In-depth Classification Performance Reporting with Fake News Example\n",
        "- false/true negatives\n",
        "- false/true positives\n",
        "- percision\n",
        "- recall\n",
        "- f1\n",
        "\n",
        "![alt text](https://d1i4t8bqe7zgj6.cloudfront.net/11-18-2016/t_1479489048302_name_20161119_zuckerberg1.jpg)\n",
        "\n",
        "Facebook has been in the news a lot for their policies on fake news... \n",
        "Maybe you believe that facebook should crack down more on fake news so it doesn't spread across the globe. Maybe you feel that they are cracking down too hard and taking down people's posts that shouldn't be taken down. *THIS* is a matter of precision versus recall!\n",
        "\n",
        "If we cared most about **precision**, we would tune our machine learning algorithm to only take down fake news stories that we were ABSOLUTELY sure was fake news. By doing this, we may possibly miss some fake news stories.\n",
        "\n",
        "If we cared most about **recall**, we would tune our machine learning algorithm to take down ANY POSSIBLE fake news stories. We would for sure take down all fake news stories but may take down some non-fake news in the process.\n",
        "\n",
        "\n",
        "Read More: [Why Accuracy Is Not Enough](https://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtRThfuPA5ki",
        "colab_type": "text"
      },
      "source": [
        "### False/True Negatives/Positives\n",
        "\n",
        "* **True Positive** - When we predicted positive/1 (fake news) and the ground truth was positive/1 (fake news).\n",
        "* **True Negative** - When we predicted negative/0 (not fake news) and the ground truth was negative/0 (not fake news).\n",
        "* **False Positive** - When we predicted positive/1 (fake news) and the ground truth was negative/0 (not fake news).\n",
        "* **False Negative** - When we predicted negative/0 (not fake news) and the ground truth was positive/1 (fake news).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-mKXbdRhtrf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def true_pos(y_truth, y_pred):\n",
        "  return np.logical_and(y_pred == 1, y_truth == 1)\n",
        "\n",
        "def true_neg(y_truth, y_pred):\n",
        "  return np.logical_and(y_pred == 0, y_truth == 0)\n",
        "\n",
        "def false_pos(y_truth, y_pred):\n",
        "  return np.logical_and(y_pred == 1, y_truth == 0)\n",
        "\n",
        "def false_neg(y_truth, y_pred):\n",
        "  return np.logical_and(y_pred == 0, y_truth == 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1cQBMMGhsLI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_model(w, b):\n",
        "    fig, (ax1) = plt.subplots(1, 1, figsize=(8, 6))\n",
        "    y_hat = logistic_predict_binary(w, b, x)\n",
        "    tp, fp = true_pos(y, y_hat).sum(), false_pos(y, y_hat).sum()\n",
        "    fn, tn = false_neg(y, y_hat).sum(), true_neg(y, y_hat).sum()\n",
        "    ax1.scatter(x[false_neg(y, y_hat)], y[false_neg(y, y_hat)], label=f\"False Neg: {fn}\", c='orange', alpha=.3)\n",
        "    ax1.scatter(x[false_pos(y, y_hat)], y[false_pos(y, y_hat)], label=f\"False Pos: {fp}\", c='orange', alpha=.3, marker=\"x\")\n",
        "    ax1.scatter(x[true_neg(y, y_hat)], y[true_neg(y, y_hat)], label=f\"True Neg: {tn}\", c='blue', marker=\"x\")\n",
        "    ax1.scatter(x[true_pos(y, y_hat)], y[true_pos(y, y_hat)], label=f\"True Pos: {tp}\", c='blue')\n",
        "    # Plot Model\n",
        "    y_hat = logistic_predict_prob(w, b, x_range)\n",
        "    predictions = logistic_predict_binary(w, b, x)\n",
        "    accuracy = (predictions == y).sum() / len(predictions)\n",
        "    ax1.plot(x_range, y_hat, color=\"r\", label='Model - Acc: {}'.format(accuracy))\n",
        "    ax1.plot(x_range, logistic_predict_binary(w, b, x_range), color=\"g\", label='Decision Boundary')\n",
        "    ax1.set_xlabel(\"Fake News Score\")\n",
        "    ax1.set_ylabel(\"Probability of Story Being Fake News\")\n",
        "    ax1.legend(loc='upper left')\n",
        "    ax1.set_title('Fake News Classifier')\n",
        "\n",
        "# Run this block to vizualize how to fit a model\n",
        "# You may have to run this cell a few times to get the plot to show\n",
        "interact(\n",
        "  plot_model,\n",
        "  w = widgets.FloatSlider(value=1, min=w_min, max=w_max, step=w_step),\n",
        "  b = widgets.FloatSlider(value=-8, min=b_min, max=b_max, step=b_step)\n",
        ")\n",
        "print(\"Adjust the parameters: w and b to fit the model to the data!\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXgDJYPTBa4O",
        "colab_type": "text"
      },
      "source": [
        "### Confusion Matrix\n",
        "- A confusion matrix is a way to visualize our model's success. In short, you want the highest numbers along the diagonal (correct guesses). We'd like 0's everywhere else!\n",
        "\n",
        "![alt text](https://miro.medium.com/max/1024/1*-BkpqhN-5fPicMifDQ0SwA.png)\n",
        "\n",
        "Adjust the model below to achieve the highest numbers along the diagonals!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9cCE_xWBeAF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_model(w, b):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "    y_hat = logistic_predict_binary(w, b, x)\n",
        "    tp, fp = true_pos(y, y_hat).sum(), false_pos(y, y_hat).sum()\n",
        "    fn, tn = false_neg(y, y_hat).sum(), true_neg(y, y_hat).sum()\n",
        "    ax1.scatter(x[false_neg(y, y_hat)], y[false_neg(y, y_hat)], label=f\"False Neg: {fn}\", c='orange', alpha=.3)\n",
        "    ax1.scatter(x[false_pos(y, y_hat)], y[false_pos(y, y_hat)], label=f\"False Pos: {fp}\", c='orange', alpha=.3, marker=\"x\")\n",
        "    ax1.scatter(x[true_neg(y, y_hat)], y[true_neg(y, y_hat)], label=f\"True Neg: {tn}\", c='blue', marker=\"x\")\n",
        "    ax1.scatter(x[true_pos(y, y_hat)], y[true_pos(y, y_hat)], label=f\"True Pos: {tp}\", c='blue')\n",
        "    # Plot Confusion Matrix\n",
        "    matrix = [\n",
        "          [tp, fp],\n",
        "          [fn, tn]\n",
        "    ]\n",
        "    df = pd.DataFrame(\n",
        "        matrix,\n",
        "        index = [\"Guessed\\nFake News\", \"Guessed\\nNot Fake News\"],\n",
        "        columns = [\"Actually Fake News\", \"Actually Not Fake News\"]\n",
        "    )\n",
        "    ax2.set_title(\"Confusion Matrix\")\n",
        "    sns.heatmap(df, annot=True, cmap=\"Blues\", ax=ax2, cbar=False)\n",
        "    # Plot Model\n",
        "    y_hat = logistic_predict_prob(w, b, x_range)\n",
        "    predictions = logistic_predict_binary(w, b, x)\n",
        "    accuracy = (predictions == y).sum() / len(predictions)\n",
        "    ax1.plot(x_range, y_hat, color=\"r\", label='Model - Acc: {}'.format(accuracy))\n",
        "    ax1.plot(x_range, logistic_predict_binary(w, b, x_range), color=\"g\", label='Decision Boundary')\n",
        "    ax1.set_xlabel(\"Fake News Score\")\n",
        "    ax1.set_ylabel(\"Probability of Story Being Fake News\")\n",
        "    ax1.legend(loc='upper left')\n",
        "    ax1.set_title('Fake News Classifier', fontsize=20)\n",
        "\n",
        "# Run this block to vizualize how to fit a model\n",
        "# You may have to run this cell a few times to get the plot to show\n",
        "interact(\n",
        "  plot_model,\n",
        "  w = widgets.FloatSlider(value=1, min=w_min, max=w_max, step=w_step),\n",
        "  b = widgets.FloatSlider(value=-8, min=b_min, max=b_max, step=b_step)\n",
        ")\n",
        "print(\"Adjust the parameters: w and b to fit the model to the data!\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVE1NwO5BRx1",
        "colab_type": "text"
      },
      "source": [
        "### Precision, Recall and F1\n",
        "\n",
        "- **Precision** - **P**(actually positive | predicted positive)\n",
        "- **Recall** - **P**(predicted positive | actually positive)\n",
        "- **F1** - A (harmonic) mean of the recall and precision"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3T_KTH2bBOw-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def recall(y_truth, y_pred):\n",
        "  return true_pos(y_truth, y_pred).sum() / (true_pos(y_truth, y_pred).sum() + false_neg(y_truth, y_pred).sum())\n",
        "\n",
        "def precision(y_truth, y_pred):\n",
        "  return true_pos(y_truth, y_pred).sum() / (true_pos(y_truth, y_pred).sum() + false_pos(y_truth, y_pred).sum())\n",
        "\n",
        "def f1(y_truth, y_pred):\n",
        "  prec = precision(y_truth, y_pred)\n",
        "  recl = recall(y_truth, y_pred)\n",
        "  return 2 * ((prec*recl)/(prec+recl))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASOj6LDXBsdl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_model(w, b):\n",
        "    fig, (ax1) = plt.subplots(1, 1, figsize=(8, 6))\n",
        "    y_hat = logistic_predict_binary(w, b, x)\n",
        "    ax1.scatter(x[false_neg(y, y_hat)], y[false_neg(y, y_hat)], label=\"False Neg\", c='orange', alpha=.3)\n",
        "    ax1.scatter(x[false_pos(y, y_hat)], y[false_pos(y, y_hat)], label=\"False Pos\", c='orange', alpha=.3, marker=\"x\")\n",
        "    ax1.scatter(x[true_neg(y, y_hat)], y[true_neg(y, y_hat)], label=\"True Neg\", c='blue', marker=\"x\")\n",
        "    ax1.scatter(x[true_pos(y, y_hat)], y[true_pos(y, y_hat)], label=\"True Pos\", c='blue')\n",
        "    # Percision and Recall\n",
        "    ax1.annotate(\"Recall: {:.2f}\".format(recall(y, y_hat)), (0, .4), fontsize=12, horizontalalignment='left')\n",
        "    ax1.annotate(\"Precision: {:.2f}\".format(precision(y, y_hat)), (0, .3), fontsize=12, horizontalalignment='left')\n",
        "    ax1.annotate(\"F1: {:.2f}\".format(f1(y, y_hat)), (0, .2), fontsize=12, horizontalalignment='left')\n",
        "    # Plot Model\n",
        "    y_hat = logistic_predict_prob(w, b, x_range)\n",
        "    predictions = logistic_predict_binary(w, b, x)\n",
        "    accuracy = (predictions == y).sum() / len(predictions)\n",
        "    ax1.plot(x_range, y_hat, color=\"r\", label='Model - Acc: {}'.format(accuracy))\n",
        "    ax1.plot(x_range, logistic_predict_binary(w, b, x_range), color=\"g\", label='Decision Boundary')\n",
        "    ax1.set_xlabel(\"Fake News Score\")\n",
        "    ax1.set_ylabel(\"Probability of Story Being Fake News\")\n",
        "    ax1.legend(loc='upper left')\n",
        "    ax1.set_title('Fake News Classifier')\n",
        "\n",
        "# Run this block to vizualize how to fit a model\n",
        "# You may have to run this cell a few times to get the plot to show\n",
        "interact(\n",
        "  plot_model,\n",
        "  w = widgets.FloatSlider(value=1, min=w_min, max=w_max, step=w_step),\n",
        "  b = widgets.FloatSlider(value=-8, min=b_min, max=b_max, step=b_step)\n",
        ")\n",
        "print(\"Adjust the parameters: w and b to fit the model to the data!\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7rJipHIIb12",
        "colab_type": "text"
      },
      "source": [
        "## Feature Interpretation\n",
        "\n",
        "In the model above, we got a w of 0.9 and b of -4.8. How can these be interpreted? Well, the log odds are a linear combination of the parameters. Meaning we can represent our logarithmic model as:\n",
        "\n",
        "$$log(\\frac{p}{1-p})=wx+b$$\n",
        "\n",
        "Which, holding b constant becomes, we find that, with $w=0.9$, a one unit increase in $x$, or rather one more month of subscription corresponds to a 0.9 higher log odds.\n",
        "\n",
        "With some algebra, this becomes:\n",
        "\n",
        "$$\\frac{p}{1-p}=e^{wx+b}$$\n",
        "\n",
        "\n",
        "one more month of subscription corresponds to a $e^{0.9}=2.5$ higher odds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOX7w0JX4TDX",
        "colab_type": "text"
      },
      "source": [
        "## Multilinear Regression: Adding Features\n",
        "\n",
        "\n",
        "You can easily add more features to the logistic regression. Same theory and code applies; we just can't really plot in the same way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lfblRoLMa10",
        "colab_type": "text"
      },
      "source": [
        "### 2D Decision Boundaries\n",
        "\n",
        "With two features, we actually can still plot our decision boundary, from the top looking down.\n",
        "\n",
        "**Todo: plot decision boundaries**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAHXIrZSJSAh",
        "colab_type": "text"
      },
      "source": [
        "## Multiclass Regression: Adding Features\n",
        "\n",
        "Logistic Regression can also be used for when your target variable has more than two classes. Same theory and code applies; we just can't really plot in the same way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ioGicxrQWU1",
        "colab_type": "text"
      },
      "source": [
        "## Challenges\n",
        "\n",
        "Enter your email below and run the cell to show that you have completed the lesson!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZPiQXdUQVRp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# These are internal libraries used to manage your submissions to the challenges\n",
        "# You won't have to install / import these in your normal usage\n",
        "# Do not change the next three lines\n",
        "!pip install pycosmos --upgrade --quiet\n",
        "from pycosmos import CosmosProject\n",
        "tamu_datathon = CosmosProject('tamu_datathon')\n",
        "\n",
        "email = \"you@email.com\" # Enter your email here\n",
        "\n",
        "# Do not change this line\n",
        "print(tamu_datathon.judge_attempt('lesson_3_completion', email, [\"Done\"]))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}